[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2026 keju authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/introduction.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Introduction to keju","text":"keju set models statistical inference Massively Parallel Reporter Assay (MPRA) data. takes raw counts assign enhancer transcription rate , optionally, effect size denotes difference transcription rates control treatment alternate treatment. package, keju filter process data fitting model. Alternatively, can provide pre-filtered counts skip keju filtering step. Please just note must provide raw unnormalized counts (, minimum, integers). note terminology, “architecture” can little bit confusing. keju, “architecture” something infer transcription rate /differential activity . “architecture” can ref/alt SNP pair (case “control condition” reference allele “alternate treatment” alternate allele), regulatory element different conditions (think drug effect comparison), etc.","code":""},{"path":"/articles/introduction.html","id":"choosing-the-correct-keju-model","dir":"Articles","previous_headings":"Introduction","what":"Choosing the correct keju model","title":"Introduction to keju","text":"Choosing correct model can somewhat confusing. keju singular model, suite models different use cases fit within , little like Russian nesting dolls. summarize next paragraphs, simplest modeling option no_motif, get benefits using keju. ’re sure model use, probably least start no_motif. motif_shrinkage slightly specialized version no_motif. architectures kind shared structure (.e., multiple architectures test transcription factor binding motif), motif_shrinkage statistical niceties may interest slightly better performance given motif-level metadata. covariate_motif_slope_intercept slightly specialized version motif_shrinkage. think covariates affecting transcription rate architectures (.e. minimal promoter choice (see paper)), covariate_motif_slope_intercept can quantify effects given motif-level covariate-level metadata. general model, default recommended starting point, no_motif. no_motif makes assumptions correlations enhancers architectures, shrinks transcription rate estimates effect size estimates towards generic standard normal prior. Use no_motif don’t concrete structure among tested architectures, many users . Even kind structure, no_motif give viable estimates, just without motif-level bells whistles. contrast, users can use motif_shrinkage exploitable structure tested architectures. example motif-level structure architectures provided Zahm et al. data, several architectures actually testing Rarb transcription factor binding motif. case, motif_shrinkage shrinks estimates towards shared motif-level mean, also provide motif-level estimates transcription rate effect sizes, just architecture-level estimates. example, example no_motif motif_shrinkage provide 18 transcription rate estimates 18 effect size estimates, one architecture. However, motif_shrinkage also provide transcription rate estimate effect size estimate motif , architecture-level estimates regularized motif-level estimates. Use motif_shrinkage kind structure among architectures. know structure qualifies, recommend just using no_motif.  Finally, specialized model covariate_motif_slope_intercept, also full keju model. covariate_motif_slope_intercept requires motif-level structure, used case covariates provided model interesting effects transcription rate want disentangle. example, Zahm et al. data, choice minimal promoter strongly affects transcription rate estimates architecture (see Figure 4 paper). Use covariate_motif_slope_intercept want covariate-level slope intercept estimates transcription rate.","code":""},{"path":[]},{"path":"/articles/introduction.html","id":"loading-the-input-data","dir":"Articles","previous_headings":"Usage","what":"Loading the input data","title":"Introduction to keju","text":"load input data , contains first 100 architectures (alphabetical order) SF_DEX paper. data also contains maximum 20 barcodes per architecture runtime’s sake. , mark control treatment (SF, denoting “Serum Free”) negative control architectures. keju uses negative controls correct global shifts differential activity, can covariate-specific manner. also provide covariates want correct effect sizes (case, ’s minimal promoter used architecture).","code":"library(keju) library(dplyr) #> Warning: package 'dplyr' was built under R version 4.4.3 #>  #> Attaching package: 'dplyr' #> The following object is masked from 'package:keju': #>  #>     filter #> The following objects are masked from 'package:stats': #>  #>     filter, lag #> The following objects are masked from 'package:base': #>  #>     intersect, setdiff, setequal, union library(stringr)  df <- readRDS(system.file(\"extdata\", \"sf_dex.rda\", package=\"keju\")) head(df) #>        X Unnamed..0 RNA_sample_number treatment DNA_sample_number #> 1  11149      11149                 1        SF                42 #> 2  19492      19492                 1        SF                42 #> 3 107905     107905                 1        SF                42 #> 4 158325     158325                 1        SF                42 #> 5 176049     176049                 1        SF                42 #> 6 207146     207146                 1        SF                42 #>                    barcode DNA_count  DNA_rpm                 architecture #> 1 GCTTCCTTGTTAGTTATTAAGATA       392 4.413638 ALX3-1:1223, 1, set2, minCMV #> 2 AAGTGTTTGCGTCAACCTTACAAA       341 3.839415 ALX3-1:1223, 1, set2, minCMV #> 3 CCTTCCCACATACTACGACATGCA       182 2.049189 ALX3-1:1223, 1, set2, minCMV #> 4 GATTCTGACTGGGATCTGAATTCC       146 1.643855 ALX3-1:1223, 1, set2, minCMV #> 5 ATTGATTAAATATAAAGGTATACC       135 1.520003 ALX3-1:1223, 1, set2, minCMV #> 6 CTTAGTTAGTTTCTCGGGACTTTG       118 1.328595 ALX3-1:1223, 1, set2, minCMV #>       class barcode_n RNA_count   RNA_rpm #> 1 candidate         1        39 1.6640826 #> 2 candidate         2        94 4.0108659 #> 3 candidate         3        34 1.4507387 #> 4 candidate         4         7 0.2986815 #> 5 candidate         5         5 0.2133439 #> 6 candidate         6        39 1.6640826 df$is_control_treatment = (df$treatment == 'SF') df$is_control_architecture = (df$class == 'scramble' | df$class == 'spacer') df$covariates = str_split_fixed(df$architecture, ', ', 4)[, 4]"},{"path":"/articles/introduction.html","id":"no_motif","dir":"Articles","previous_headings":"Usage","what":"no_motif","title":"Introduction to keju","text":"section details running keju no_motif setting. motif_shrinkage covariate_motif_slope_intercept, sections . Note processing step also changes different models. , build keju object data. case, control treatment (SF) alternate treatment (Dex), interested differential activity two (hence, infer_differential_activity=TRUE). single treatment, interested baseline transcription rate single treatment, simply set infer_differential_activity = FALSE. Note DNA RNA counts matched (barcode, RNA batch), every argument keju_from_counts length N vector (castable length N vector), N number observations. experiment, 100 architectures, architecture maximum 20 barcodes, observations two RNA batches. comes 8230 observations, observation barcode RNA batch, necessarily DNA batch DNA counts (can !). first time run keju, basilisk install conda environment necessary prerequisites . set output directory model outputs. keju automatically create directory finds missing. Nicely processed estimates output . Since interested no_motif model subsection, set model=‘no_motif’. Fitting toy dataset takes minutes, can take hours full size dataset. Generally speaking, keju also requires use four cores, one per fit chain. Otherwise fit chains serially (see cmdstanr documentation ).","code":"# build the keju object keju <- keju_from_counts(     barcode=df$barcode,     R = df$RNA_count,     D = df$DNA_count,     batch = df$RNA_sample_number,     dna_batch = df$DNA_sample_number,     architecture = df$architecture,     treatment = df$treatment,     is_control_treatment = df$is_control_treatment,     is_control_architecture = df$is_control_architecture,     covariates=df$covariates )  # optional filtering step keju <- keju::filter(keju) #> Converting 425 NA values to zero #> Filtering 30 architectures with insufficient barcodes.  # process the filtered data to fit no_motif keju <- use_no_motif(keju, infer_differential_activity = TRUE) output_dir <- 'keju_vignette'  # fit no_motif keju <- keju_fit(keju, output_dir=paste0(output_dir, '/no_motif'), model='no_motif', infer_differential_activity=TRUE) #> Running MCMC with 4 parallel chains... #>  #> Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 1 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 4 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 1 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 3 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 2 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 1 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 4 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 3 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 2 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 1 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 4 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 3 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 2 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 4 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 3 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 2 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 3 finished in 94.2 seconds. #> Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 2 finished in 145.9 seconds. #> Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 4 finished in 164.9 seconds. #> Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 1 finished in 182.4 seconds. #>  #> All 4 chains finished successfully. #> Mean chain execution time: 146.9 seconds. #> Total execution time: 182.5 seconds. #> Warning: 8 of 4000 (0.0%) transitions hit the maximum treedepth limit of 10. #> See https://mc-stan.org/misc/warnings for details. #>  #> Warning: 8 of 4000 (0.0%) transitions hit the maximum treedepth limit of 10. #> See https://mc-stan.org/misc/warnings for details. #> $num_divergent #> [1] 0 0 0 0 #>  #> $num_max_treedepth #> [1] 5 0 0 3 #>  #> $ebfmi #> [1] 0.9536814 1.0435768 0.9728513 1.0570126 #> Warning: Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated."},{"path":"/articles/introduction.html","id":"motif_shrinkage","dir":"Articles","previous_headings":"Usage","what":"motif_shrinkage","title":"Introduction to keju","text":"section details usage model motif_shrinkage. Note preprocessing fitting steps change slightly. using motif-level information, need provide appropriate metadata preprocessing. Note , providing motif-level information keju_from_counts. still need filter counts, processing function call different (use_motif_shrinkage instead use_no_motif). Finally, specify motif_shrinkage model argument.","code":"df$motif = str_split_fixed(df$architecture, ':', 2)[, 1]  keju_ms <- keju_from_counts(     barcode=df$barcode,     R = df$RNA_count,     D = df$DNA_count,     batch = df$RNA_sample_number,     dna_batch = df$DNA_sample_number,     architecture = df$architecture,     treatment = df$treatment,     is_control_treatment = df$is_control_treatment,     is_control_architecture = df$is_control_architecture,     covariates = df$covariates,     motif = df$motif ) keju_ms <- keju::filter(keju_ms) #> Converting 425 NA values to zero #> Filtering 30 architectures with insufficient barcodes. keju_ms <- use_motif_shrinkage(keju_ms, infer_differential_activity = TRUE) keju_ms <- keju_fit(keju_ms, paste0(output_dir, '/motif_shrinkage'), model='motif_shrinkage', infer_differential_activity=TRUE) #> Running MCMC with 4 parallel chains... #>  #> Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 3 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 3 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 2 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 1 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 4 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 3 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 2 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 1 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 4 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 3 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 2 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 1 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 4 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 2 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 1 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 4 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 3 finished in 99.8 seconds. #> Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 2 finished in 106.2 seconds. #> Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 1 finished in 108.8 seconds. #> Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 4 finished in 109.8 seconds. #>  #> All 4 chains finished successfully. #> Mean chain execution time: 106.2 seconds. #> Total execution time: 110.0 seconds. #>  #> $num_divergent #> [1] 0 0 0 0 #>  #> $num_max_treedepth #> [1] 0 0 0 0 #>  #> $ebfmi #> [1] 0.9976810 1.0551858 0.9785017 1.0254429 #> Warning: Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated."},{"path":"/articles/introduction.html","id":"covariate_motif_slope_intercept","dir":"Articles","previous_headings":"Usage","what":"covariate_motif_slope_intercept","title":"Introduction to keju","text":"section details usage model covariate_motif_slope_intercept. start creating keju object filtering counts. Maybe predictably, preprocessing function use use_covariate_slope_intercept instead use_no_motif use_motif_shrinkage. fit model, specifying covariate_motif_slope_intercept model argument.","code":"keju_cmsi <- keju_from_counts(     barcode=df$barcode,     R = df$RNA_count,     D = df$DNA_count,     batch = df$RNA_sample_number,     dna_batch = df$DNA_sample_number,     architecture = df$architecture,     treatment = df$treatment,     is_control_treatment = df$is_control_treatment,     is_control_architecture = df$is_control_architecture,     covariates = df$covariates,     motif = df$motif )  keju_cmsi <- keju::filter(keju_cmsi) #> Converting 425 NA values to zero #> Filtering 30 architectures with insufficient barcodes. keju_cmsi <- use_covariate_slope_intercept(keju_cmsi, infer_differential_activity = TRUE) keju_cmsi <- keju_fit(keju_cmsi, paste0(output_dir, '/covariate_motif_slope_intercept'),                   model='covariate_motif_slope_intercept',                   infer_differential_activity=TRUE) #> Running MCMC with 4 parallel chains... #>  #> Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup)  #> Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup)  #> Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup)  #> Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup)  #> Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup)  #> Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 1 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup)  #> Chain 3 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 1 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 2 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 4 Iteration:  600 / 2000 [ 30%]  (Warmup)  #> Chain 3 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 1 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 2 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 4 Iteration:  700 / 2000 [ 35%]  (Warmup)  #> Chain 3 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 2 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 1 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 4 Iteration:  800 / 2000 [ 40%]  (Warmup)  #> Chain 3 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 2 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 4 Iteration:  900 / 2000 [ 45%]  (Warmup)  #> Chain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup)  #> Chain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling)  #> Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling)  #> Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling)  #> Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling)  #> Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling)  #> Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling)  #> Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling)  #> Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling)  #> Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling)  #> Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 1 finished in 102.3 seconds. #> Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling)  #> Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 3 finished in 105.1 seconds. #> Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 2 finished in 106.5 seconds. #> Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling)  #> Chain 4 finished in 108.4 seconds. #>  #> All 4 chains finished successfully. #> Mean chain execution time: 105.6 seconds. #> Total execution time: 108.5 seconds. #>  #> $num_divergent #> [1] 0 0 0 0 #>  #> $num_max_treedepth #> [1] 0 0 0 0 #>  #> $ebfmi #> [1] 1.0016497 0.9916731 0.9401170 0.9802777 #> Warning: Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated. #> Setting row names on a tibble is deprecated."},{"path":"/articles/introduction.html","id":"accessing-estimates","dir":"Articles","previous_headings":"","what":"Accessing estimates","title":"Introduction to keju","text":"keju provides transcription rate estimates (alpha) effect size/differential activity estimates (beta). can access estimates shown . important columns mean (mean estimate), sd (standard deviation estimate), is_significant. Note estimates fairly consistent model type, even without motif information. Using motif information improves inference, strictly necessary keju performance.   models motif-level information (motif_shrinkage covariate_motif_slope_intercept), can access motif-level estimates manner shown . Finally, covariate_motif_slope_intercept, can access covariate-level effects transcription rate manner: Thanks using keju! Email albertsxue@gmail.com questions.","code":"# alpha estimates head(keju$alphas_estimate) #> # A tibble: 6 × 11 #>   variable   mean median     sd    mad     q5     q95  rhat ess_bulk ess_tail #>   <chr>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl> <dbl>    <dbl>    <dbl> #> 1 alpha[1] -0.383 -0.385 0.0884 0.0877 -0.524 -0.236   1.00    5118.    2706. #> 2 alpha[2] -0.177 -0.178 0.0829 0.0846 -0.315 -0.0413  1.00    5032.    3009. #> 3 alpha[3] -0.913 -0.913 0.0850 0.0855 -1.05  -0.772   1.00    4492.    2971. #> 4 alpha[4] -0.658 -0.657 0.0756 0.0756 -0.781 -0.531   1.00    4643.    3348. #> 5 alpha[5] -0.444 -0.445 0.0842 0.0819 -0.580 -0.306   1.00    4636.    2599. #> 6 alpha[6] -0.692 -0.692 0.0711 0.0717 -0.809 -0.576   1.00    5498.    2871. #> # ℹ 1 more variable: architecture <chr>  # beta estimates head(keju$betas_estimate) #> # A tibble: 6 × 12 #>   variable   mean median    sd   mad    q5    q95  rhat ess_bulk ess_tail #>   <chr>     <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>    <dbl>    <dbl> #> 1 beta[1]  -1.18  -1.18  0.266 0.264 -1.61 -0.739  1.00    2019.    2899. #> 2 beta[2]  -1.54  -1.54  0.261 0.264 -1.96 -1.10   1.00    1930.    2502. #> 3 beta[3]  -0.576 -0.572 0.275 0.280 -1.03 -0.123  1.00    2019.    2814. #> 4 beta[4]  -1.30  -1.30  0.269 0.263 -1.75 -0.855  1.00    2105.    2714. #> 5 beta[5]  -1.44  -1.44  0.258 0.259 -1.85 -1.01   1.01    1979.    2244. #> 6 beta[6]  -1.00  -0.997 0.255 0.255 -1.43 -0.593  1.00    2203.    2564. #> # ℹ 2 more variables: is_significant <lgl>, architecture <chr> p1 <- ggplot(mapping=aes(x=keju$alphas_estimate$mean, y=keju_ms$alphas_estimate$mean)) + geom_point() + geom_abline(slope=1, intercept=0, color='red') print(p1) p2 <- ggplot(mapping=aes(x=keju$betas_estimate$mean, y=keju_ms$betas_estimate$mean)) + geom_point() + geom_abline(slope=1, intercept=0, color='red') print(p2) # transcription rate motif estimates head(keju_ms$alpha_motifs_estimate) #> # A tibble: 6 × 11 #>   variable      mean median     sd    mad     q5     q95  rhat ess_bulk ess_tail #>   <chr>        <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl> <dbl>    <dbl>    <dbl> #> 1 motif_tran… -0.169 -0.176 0.735  0.696  -1.36   1.06   1.00     4956.    3093. #> 2 motif_tran… -0.518 -0.525 0.308  0.283  -0.998 -0.0143 1.00     3380.    2506. #> 3 motif_tran… -0.731 -0.733 0.0907 0.0882 -0.878 -0.579  1.000    5209.    2660. #> 4 motif_tran… -0.672 -0.672 0.0992 0.0958 -0.838 -0.507  1.00     4353.    3019. #> 5 motif_tran…  0.774  0.776 0.208  0.206   0.439  1.11   1.00     4720.    2813. #> 6 motif_tran…  1.19   1.21  0.311  0.285   0.663  1.69   1.00     4029.    2407. #> # ℹ 1 more variable: motif <chr>  # effect size motif estimates head(keju_ms$beta_motifs_estimate) #> # A tibble: 6 × 12 #>   variable         mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail #>   <chr>           <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl> #> 1 motif_effect[… -0.547 -0.599 0.820 0.809 -1.80   0.855  1.00   3361.     2579. #> 2 motif_effect[… -0.983 -1.000 0.468 0.452 -1.73  -0.189  1.00    213.      538. #> 3 motif_effect[…  0.934  0.930 0.357 0.368  0.353  1.53   1.02     97.6     192. #> 4 motif_effect[…  1.44   1.44  0.362 0.376  0.850  2.04   1.02     97.8     179. #> 5 motif_effect[… -1.04  -1.04  0.363 0.381 -1.62  -0.440  1.01    101.      188. #> 6 motif_effect[… -1.21  -1.21  0.388 0.394 -1.85  -0.574  1.01    119.      281. #> # ℹ 2 more variables: is_significant <lgl>, motif <chr> # intercept estimates head(keju_cmsi$covariate_intercept_estimate) #> # A tibble: 3 × 11 #>   variable       mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail #>   <chr>         <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl> #> 1 intercept[1]  0.461  0.464 0.538 0.529 -0.417 1.34    1.01     634.    1245. #> 2 intercept[2] -0.397 -0.402 0.285 0.280 -0.866 0.0616  1.01     742.    1434. #> 3 intercept[3] -0.333 -0.331 0.278 0.266 -0.791 0.130   1.01     729.    1541. #> # ℹ 1 more variable: covariate <chr>  # slope estimates head(keju_cmsi$covariate_slope_estimate) #> # A tibble: 3 × 11 #>   variable   mean median    sd   mad      q5   q95  rhat ess_bulk ess_tail #>   <chr>     <dbl>  <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>    <dbl>    <dbl> #> 1 slope[1]  0.717  0.659 0.476 0.462  0.0395 1.61   1.00    1076.    1509. #> 2 slope[2] -0.142 -0.173 0.264 0.259 -0.514  0.335  1.00    1224.    1820. #> 3 slope[3] -0.158 -0.185 0.261 0.252 -0.528  0.321  1.00    1219.    2013. #> # ℹ 1 more variable: covariate <chr>"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Albert Xue. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Xue (2026). keju: Analyze Massively Parallel Reporter Assay Data. R package version 0.0.0.9000, http://pimentellab.com/keju/, https://github.com/pimentellab/keju.","code":"@Manual{,   title = {keju: Analyze Massively Parallel Reporter Assay Data},   author = {Albert Xue},   year = {2026},   note = {R package version 0.0.0.9000, http://pimentellab.com/keju/},   url = {https://github.com/pimentellab/keju}, }"},{"path":"/index.html","id":"keju","dir":"","previous_headings":"","what":"Analyze Massively Parallel Reporter Assay Data","title":"Analyze Massively Parallel Reporter Assay Data","text":"keju R package statistical analysis Massively Parallel Reporter Assay (MPRA) data, outputs transcription rate estimates differential activity estimates batch-specific uncertainty quantification.","code":""},{"path":[]},{"path":"/index.html","id":"r-package-installation","dir":"","previous_headings":"Installation","what":"R package installation","title":"Analyze Massively Parallel Reporter Assay Data","text":"keju relies cmdstanr basilisk. install cmdstanr, run compiler requirements can seen stan-dev. run issues installation, please ensure gcc version > 5. Additionally, run issues TBB library, try downgrading CmdStan version 2.33.1. keju performs sparse matrix processing steps require python. result, R API thin wrapper around python calls, require python==3.13.3, numpy==2.2.5, pandas==2.2.3, formulaic==1.1.1. use basilisk install new python environment packages. See basilisk, specifically basilisk::configureBasiliskEnv(), information details installation location. install basilisk, run installing cmdstanr basilisk, install keju running","code":"install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))  # use cmdstanr to install CmdStan, this requires a working C++ toolchain and compiler library(cmdstanr) install_cmdstan(cores = 2) if (!require(\"BiocManager\", quietly = TRUE))     install.packages(\"BiocManager\")  BiocManager::install(\"basilisk\") if (!requireNamespace(\"remotes\", quietly = TRUE)) {   install.packages(\"remotes\") }  remotes::install_github(\"pimentellab/keju\") library(keju)"},{"path":"/index.html","id":"using-keju","dir":"","previous_headings":"","what":"Using keju","title":"Analyze Massively Parallel Reporter Assay Data","text":"introductory vignette can found . run problems, please submit issue github email albertsxue@gmail.com.","code":""},{"path":"/index.html","id":"choosing-the-correct-keju-model","dir":"","previous_headings":"Using keju","what":"Choosing the correct keju model","title":"Analyze Massively Parallel Reporter Assay Data","text":"keju singular model, suite models different use cases fit within , little like Russian nesting dolls. result, choosing correct model use case can confusing. simplest modeling option no_motif, get benefits using keju. ’re sure model use, probably least start no_motif. motif_shrinkage slightly specialized version no_motif. architectures kind shared structure (.e., multiple architectures test transcription factor binding motif), motif_shrinkage statistical niceties may interest slightly better performance given motif-level metadata. covariate_motif_slope_intercept slightly specialized version motif_shrinkage. think covariates affecting transcription rate architectures (.e. minimal promoter choice (see paper)), covariate_motif_slope_intercept can quantify effects given motif-level covariate-level metadata. questions, vignette may helpful.","code":""},{"path":"/reference/filter.html","id":null,"dir":"Reference","previous_headings":"","what":"Filters architectures without at least an average of barcode_threshold barcodes per batch with DNA counts > dna_threshold and RNA counts > rna_threshold. — filter","title":"Filters architectures without at least an average of barcode_threshold barcodes per batch with DNA counts > dna_threshold and RNA counts > rna_threshold. — filter","text":"Filters architectures without least average barcode_threshold barcodes per batch DNA counts > dna_threshold RNA counts > rna_threshold.","code":""},{"path":"/reference/filter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filters architectures without at least an average of barcode_threshold barcodes per batch with DNA counts > dna_threshold and RNA counts > rna_threshold. — filter","text":"","code":"filter(keju, dna_threshold = 5, rna_threshold = 5, barcode_threshold = 10)"},{"path":"/reference/filter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filters architectures without at least an average of barcode_threshold barcodes per batch with DNA counts > dna_threshold and RNA counts > rna_threshold. — filter","text":"keju keju object containing counts dna_threshold minimum DNA counts per barcode rna_threshold minimum RNA counts per barcode barcode_threshold minimum average barcodes per batch","code":""},{"path":"/reference/filter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filters architectures without at least an average of barcode_threshold barcodes per batch with DNA counts > dna_threshold and RNA counts > rna_threshold. — filter","text":"keju object filtered_counts object","code":""},{"path":"/reference/keju-package.html","id":null,"dir":"Reference","previous_headings":"","what":"keju: Analyze Massively Parallel Reporter Assay Data — keju-package","title":"keju: Analyze Massively Parallel Reporter Assay Data — keju-package","text":"Bayesian hierarchical model estimating transcription rate differential activity Massively Parallel Reporter Assay data.","code":""},{"path":[]},{"path":"/reference/keju-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"keju: Analyze Massively Parallel Reporter Assay Data — keju-package","text":"Maintainer: Albert Xue albertsxue@gmail.com","code":""},{"path":"/reference/keju_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Fits keju model to data. — keju_fit","title":"Fits keju model to data. — keju_fit","text":"Fits keju model data.","code":""},{"path":"/reference/keju_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fits keju model to data. — keju_fit","text":"","code":"keju_fit(   keju,   output_dir,   model = \"no_motif\",   infer_differential_activity = FALSE,   seed = 1,   chains = 4,   parallel_chains = 4 )"},{"path":"/reference/keju_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fits keju model to data. — keju_fit","text":"keju filtered processed keju object output_dir directory output files model model type. Must one 'no_motif', 'motif_shrinkage', 'covariate_motif_slope_intercept. infer_differential_activity boolean use effect size estimation control alternate treatment seed random seed MCMC sampling. See https://mc-stan.org/cmdstanr/reference/model-method-sample.html chains number Markov chains use.  See https://mc-stan.org/cmdstanr/reference/model-method-sample.html parallel_chains maximum number MCMC chains run parallel. See https://mc-stan.org/cmdstanr/reference/model-method-sample.html","code":""},{"path":"/reference/keju_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fits keju model to data. — keju_fit","text":"fitted keju object","code":""},{"path":"/reference/keju_from_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Create keju object from count data. Counts and metadata should be matched by barcode and batch. — keju_from_counts","title":"Create keju object from count data. Counts and metadata should be matched by barcode and batch. — keju_from_counts","text":"Create keju object count data. Counts metadata matched barcode batch.","code":""},{"path":"/reference/keju_from_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create keju object from count data. Counts and metadata should be matched by barcode and batch. — keju_from_counts","text":"","code":"keju_from_counts(   barcode,   R,   D,   batch,   dna_batch,   architecture,   treatment,   is_control_treatment = FALSE,   is_control_architecture = FALSE,   covariates = \"correction\",   motif = NULL )"},{"path":"/reference/keju_from_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create keju object from count data. Counts and metadata should be matched by barcode and batch. — keju_from_counts","text":"barcode length N vector barcodes R length N vector RNA counts D length N vector DNA counts batch length N vector denoting RNA batches dna_batch length N vector denoting DNA batches architecture length N vector enhancer architectures names. Transcription rate effect size estimates fit . treatment length N vector detailing treatments. is_control_treatment length N boolean vector denoting control treatment. is_control_architecture length N boolean vector denoting architectures negative controls. covariates length N vector denoting covariates used effect size correction potentially slope/intercept fitting. none, default vector ones length N, perform global effect size correction. motif length N vector denoting motifs shared across architectures. Used shrink architecture-level estimates shared motif-level estimate. Necessary motif_shrinkage covariate_motif_slope_intercept models.","code":""},{"path":"/reference/keju_from_counts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create keju object from count data. Counts and metadata should be matched by barcode and batch. — keju_from_counts","text":"initialized keju object counts object","code":""},{"path":"/reference/pretty_summarize.html","id":null,"dir":"Reference","previous_headings":"","what":"Processes parameter estimates using existing metadata from the keju object. Calls significance for effect sizes and writes to output folder. — pretty_summarize","title":"Processes parameter estimates using existing metadata from the keju object. Calls significance for effect sizes and writes to output folder. — pretty_summarize","text":"Processes parameter estimates using existing metadata keju object. Calls significance effect sizes writes output folder.","code":""},{"path":"/reference/pretty_summarize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Processes parameter estimates using existing metadata from the keju object. Calls significance for effect sizes and writes to output folder. — pretty_summarize","text":"","code":"pretty_summarize(keju, model, output_folder, infer_differential_activity)"},{"path":"/reference/pretty_summarize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Processes parameter estimates using existing metadata from the keju object. Calls significance for effect sizes and writes to output folder. — pretty_summarize","text":"keju fitted keju object model type model fit keju output_folder folder output estimate csv files infer_differential_activity boolean use effect size estimation control alternate treatment","code":""},{"path":"/reference/use_covariate_slope_intercept.html","id":null,"dir":"Reference","previous_headings":"","what":"Process data to fit keju model with motif-level shrinkage and covariate-level effects on transcription rate. — use_covariate_slope_intercept","title":"Process data to fit keju model with motif-level shrinkage and covariate-level effects on transcription rate. — use_covariate_slope_intercept","text":"Process data fit keju model motif-level shrinkage covariate-level effects transcription rate.","code":""},{"path":"/reference/use_covariate_slope_intercept.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process data to fit keju model with motif-level shrinkage and covariate-level effects on transcription rate. — use_covariate_slope_intercept","text":"","code":"use_covariate_slope_intercept(   keju,   G = 50,   infer_differential_activity = FALSE )"},{"path":"/reference/use_covariate_slope_intercept.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process data to fit keju model with motif-level shrinkage and covariate-level effects on transcription rate. — use_covariate_slope_intercept","text":"keju keju object containing counts filtered_counts object. G number enhancers per dispersion parameter infer_differential_activity boolean use effect size estimation control alternate treatment","code":""},{"path":"/reference/use_covariate_slope_intercept.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process data to fit keju model with motif-level shrinkage and covariate-level effects on transcription rate. — use_covariate_slope_intercept","text":"keju object processed covariate_motif_slope_intercept model","code":""},{"path":"/reference/use_motif_shrinkage.html","id":null,"dir":"Reference","previous_headings":"","what":"Process counts to fit keju model with motif-level shrinkage. — use_motif_shrinkage","title":"Process counts to fit keju model with motif-level shrinkage. — use_motif_shrinkage","text":"Process counts fit keju model motif-level shrinkage.","code":""},{"path":"/reference/use_motif_shrinkage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process counts to fit keju model with motif-level shrinkage. — use_motif_shrinkage","text":"","code":"use_motif_shrinkage(keju, G = 50, infer_differential_activity = FALSE)"},{"path":"/reference/use_motif_shrinkage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process counts to fit keju model with motif-level shrinkage. — use_motif_shrinkage","text":"keju keju object containing counts filtered_counts object. G number enhancers per dispersion parameter infer_differential_activity boolean use effect size estimation control alternate treatment","code":""},{"path":"/reference/use_motif_shrinkage.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process counts to fit keju model with motif-level shrinkage. — use_motif_shrinkage","text":"keju object processed motif_shrinkage model","code":""},{"path":"/reference/use_no_motif.html","id":null,"dir":"Reference","previous_headings":"","what":"Process data to fit keju model without motif-level shrinkage. Prefers to run on filtered_counts, but will run on raw counts. — use_no_motif","title":"Process data to fit keju model without motif-level shrinkage. Prefers to run on filtered_counts, but will run on raw counts. — use_no_motif","text":"Process data fit keju model without motif-level shrinkage. Prefers run filtered_counts, run raw counts.","code":""},{"path":"/reference/use_no_motif.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process data to fit keju model without motif-level shrinkage. Prefers to run on filtered_counts, but will run on raw counts. — use_no_motif","text":"","code":"use_no_motif(keju, G = 50, infer_differential_activity = FALSE)"},{"path":"/reference/use_no_motif.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process data to fit keju model without motif-level shrinkage. Prefers to run on filtered_counts, but will run on raw counts. — use_no_motif","text":"keju keju object containing counts G number enhancers per dispersion parameter infer_differential_activity boolean use effect size estimation control alternate treatment","code":""},{"path":"/reference/use_no_motif.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process data to fit keju model without motif-level shrinkage. Prefers to run on filtered_counts, but will run on raw counts. — use_no_motif","text":"keju object processed no_motif model","code":""}]
